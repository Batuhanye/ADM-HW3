{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADM-HW3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the imports needed for the following sections of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"C:\\Users\\paolo\\Documents\\Data Science\\ADM\\data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of movies\n",
    "\n",
    "This first subtask consists in parsing the html page and getting the complete list of urls.\n",
    "For this task we created the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movies_links(filepath):\n",
    "    \"\"\"\n",
    "    Takes in input the filepath of the html file containing the links of the pages to download,\n",
    "    parses it and returns a list containing the links.\n",
    "    :param filepath: path of the html file containing all the links of the wikipedia pages\n",
    "    :return: list of links\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\", encoding='utf-8') as f:\n",
    "        html = f.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    links = []\n",
    "    for a in soup.select('a'):\n",
    "        links.append(a.get('href'))\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to crawl specific informations from each wikipedia page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need a function to download the pages.\n",
    "During the download process, this function also creates a dict to associate to each movie its url, and then stores it in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pages(links, index_to_link_path):\n",
    "    \"\"\"\n",
    "    Iterate over the list of links, downloads each html page and stores it physically.\n",
    "    :param links: list of links of wikipedia pages of movies\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    # opening/creating a json file that associates to each file the respective link\n",
    "    try:\n",
    "        with open(index_to_link_path, 'r') as f:\n",
    "            index_to_link = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        index_to_link = {}\n",
    "\n",
    "    i = 0\n",
    "    while i < len(links):\n",
    "        try:\n",
    "            response = requests.get(links[i])\n",
    "            filename = \"article_\" + str(i) + \".html\"\n",
    "            filepath = \"data/movies/\" + filename\n",
    "            if response.status_code == 200:\n",
    "                with open(filepath, \"w\", encoding='utf-8') as f:\n",
    "                    f.write(response.text)\n",
    "                    # create a file which associates to each movie the url. We will need it for the final print of the results table\n",
    "                    index_to_link[i] = links[i]\n",
    "            i += 1\n",
    "            # wait a random interval of seconds before performing the next request\n",
    "            t = random.randint(1, 5)\n",
    "            time.sleep(t)\n",
    "        except Exception as e:\n",
    "            time.sleep(60 * 20)\n",
    "            print(e)\n",
    "\n",
    "    with open(index_to_link_path, 'w') as out:\n",
    "        json.dump(index_to_link, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collection functions for getting the links and downloading the pages have been grouped in the **collector_utils.py** file. In the **collector.py** has been created the function *collect()* that makes the collection process start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import collector_utils as cu\n",
    "\n",
    "\n",
    "def collect(html_filepath, index_to_link_path):\n",
    "    \"\"\"\n",
    "    Uses the functions in collector_utils to retrieve and download the wikipedia pages\n",
    "    :param html_filepath: path of the html file containing all the links to the wikipedia pages\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    links = cu.get_movies_links(html_filepath)\n",
    "    cu.download_pages(links, index_to_link_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the collection process we need to define some paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_html_path = data_path + r'\\movies1.html'\n",
    "index_to_link_path = data_path + r'\\index_to_link.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector.collect(movies_html_path, index_to_link_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the movie to url association hasn't been made during the downloading process, here is a function to create the json file afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def associate_urls(movies_path, index_to_link_path):\n",
    "    \"\"\"\n",
    "    Creates a json file that associates to the index of each movie the relative url.\n",
    "    :param movies_path: path of the folder containing the movies html files.\n",
    "    :param index_to_link_path: path to the json file where to store the urls.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    movies = os.listdir(movies_path)\n",
    "    try:\n",
    "        with open(index_to_link_path, 'r') as f:\n",
    "            index_to_link = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        index_to_link = {}\n",
    "\n",
    "    for movie in movies:\n",
    "        i = re.findall(r'\\d+', movie)[0]\n",
    "        if i in index_to_link:\n",
    "            continue\n",
    "        print(i)\n",
    "        movie_path = movies_path + '/' + movie\n",
    "        with open(movie_path, \"r\", encoding='utf-8') as f:\n",
    "            html = f.read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        canonical_links = soup.find_all(\"link\", {\"rel\": \"canonical\"})\n",
    "        url = canonical_links[0].get('href')\n",
    "        index_to_link[i] = url\n",
    "\n",
    "    with open(index_to_link_path, 'w') as out:\n",
    "        json.dump(index_to_link, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have downloaded and stored all the wikipedia pages.\n",
    "Now it's time to parse each of them and store the required informations into tsv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each page we need to parse **Title**, **Intro**, **Plot**, **Infobox**. Here are the functions to do it. All of them just take in input the *soup* for the html page we need to parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(soup):\n",
    "    \"\"\"\n",
    "    :param soup:\n",
    "    :return: cleaned up title of the movie page\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    title = soup.title.text\n",
    "    # most page titles contain the '- Wikipedia' substring, so we are going to remove it.\n",
    "    title = title.replace('- Wikipedia', '')\n",
    "    # Another substring pattern repeated a lot of times is the presence of parentheses generally\n",
    "    # containing the substring 'film', or the year of creation (which we are going to extrapolate\n",
    "    # later from the infobox. So we are going to remove them too.\n",
    "    title = re.sub('\\(.*?\\)', '', title)\n",
    "    # As finishing touch, we are going to remove any possible extra spaces at the beginning and end.\n",
    "    title = title.strip()\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intro(soup):\n",
    "    \"\"\"\n",
    "    Retrieves and returns the intro, from the html page, which is generally\n",
    "    composed by all the html paragraphs before the first not <p> tag.\n",
    "    :param soup\n",
    "    :return: intro of the movie page\n",
    "    \"\"\"\n",
    "    infobox = soup.find('table', class_='infobox')\n",
    "    if infobox is not None:\n",
    "        p = infobox.find_next_sibling('p')\n",
    "    else:\n",
    "        p = soup.p\n",
    "    intro = p.text\n",
    "    next_sib = p.find_next_sibling()\n",
    "    while next_sib and next_sib.name == 'p':\n",
    "        intro += next_sib.text\n",
    "        next_sib = next_sib.find_next_sibling()\n",
    "    return intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot(soup):\n",
    "    \"\"\"\n",
    "    Retrieves and returns the plot, from the html page, which is generally\n",
    "    composed by all the html paragraphs after the first h tag containing the \"Plot\" string in it\n",
    "    and before the next tag different from <p>.\n",
    "    :param soup\n",
    "    :return: plot of the movie page\n",
    "    \"\"\"\n",
    "    plot = \"\"\n",
    "    h_list = soup.find_all(['h1', 'h2', 'h3', 'h4'])\n",
    "    for h in h_list:\n",
    "        if 'Plot' in h.text or 'plot' in h.text:\n",
    "            plot_h = h\n",
    "            siblings = plot_h.find_next_siblings()\n",
    "            for sib in siblings:\n",
    "                if sib.name != 'p':\n",
    "                    break\n",
    "                plot += sib.text\n",
    "            break\n",
    "    if plot == \"\":\n",
    "        return \"NA\"\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the infobox we also need to create a *dict* which associates to each infobox field the name of the field which will be used in the tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "infobox_keys = {\n",
    "    \"Directed by\": \"director\",\n",
    "    \"Produced by\": \"producer\",\n",
    "    \"Written by\": \"writer\",\n",
    "    \"Starring\": \"starring\",\n",
    "    \"Music by\": \"music\",\n",
    "    \"Release date\": \"release date\",\n",
    "    \"Running time\": \"runtime\",\n",
    "    \"Country\": \"country\",\n",
    "    \"Language\": \"language\",\n",
    "    \"Budget\": \"budget\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a function to split the names which result unified after parsing them (e.g. GarwoodMarguerite -> Garwood Marguerite)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_names(s):\n",
    "    \"\"\"\n",
    "    Takes a string generally obtained from the infobox and splits it if the are\n",
    "    names unified (e.g. GarwoodMarguerite -> Garwood Marguerite)\n",
    "    :param s: string\n",
    "    :return: new string\n",
    "    \"\"\"\n",
    "    for i in range(1, len(s)):\n",
    "        if s[i].isupper() and s[i - 1] != ' ':\n",
    "            s = s[:i] + ' ' + s[i:]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_infobox(soup):\n",
    "    \"\"\"\n",
    "    :param soup\n",
    "    :return: dictionary containing the title of the movie, a list of fields and a list of values\n",
    "    \"\"\"\n",
    "    infobox_dict = dict()\n",
    "    infobox = soup.find('table', class_='infobox')\n",
    "    infobox_dict['fields'] = []\n",
    "    infobox_dict['values'] = []\n",
    "\n",
    "    if infobox is not None:\n",
    "        # find and save the film_name\n",
    "        infobox_dict['fields'].append('film_name')\n",
    "        if infobox.th is not None:\n",
    "            infobox_dict['values'].append(infobox.th.text)\n",
    "        else:\n",
    "            # if not find, save NA\n",
    "            infobox_dict['values'].append('NA')\n",
    "\n",
    "        # find all the rows of the infobox\n",
    "        info = infobox.find_all('tr')[2:]\n",
    "\n",
    "        for tr in info:\n",
    "            # for each row, check if its th is note None and if the field is needed\n",
    "            # then eventually store it\n",
    "            if (tr.th is not None) and (tr.th.text in infobox_keys.keys()):\n",
    "                field = infobox_keys[tr.th.text]\n",
    "                infobox_dict['fields'].append(field)\n",
    "                infobox_dict['values'].append(split_names(tr.td.text))\n",
    "\n",
    "        # for all the field needed but not found, set their value as NA\n",
    "        for f in infobox_keys.values():\n",
    "            if f not in infobox_dict['fields']:\n",
    "                infobox_dict['fields'].append(f)\n",
    "                infobox_dict['values'].append('NA')\n",
    "\n",
    "    return infobox_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a function that takes in input all the info for a movie and stores them into a tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tsv(title, intro, plot, infobox, filepath):\n",
    "    import csv\n",
    "    \"\"\"\n",
    "    Saves a .tsv file containing title, intro, plot and infobox.\n",
    "    :param intro:\n",
    "    :param plot:\n",
    "    :param infobox:\n",
    "    :param filepath:\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    fields = ['title', 'intro', 'plot'] + infobox['fields']\n",
    "    values = [title, intro, plot] + infobox['values']\n",
    "    with open(filepath, 'w', encoding='utf-8') as out:\n",
    "        tsv_writer = csv.writer(out, delimiter='\\t')\n",
    "        tsv_writer.writerow(fields)\n",
    "        tsv_writer.writerow(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the previous functions from 1.3 have been grouped into **parser_utils.py**.\n",
    "They are called from the next function, *parse_file*, saved into **parser.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.parser_utils import *\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def parse_file(filepath, i, tsv_path):\n",
    "    \"\"\"\n",
    "    Takes in input the filepath and the index of the file to parse and creates a tsv file\n",
    "    containing title, intro, plot and infobox information taken from the html.\n",
    "    :param filepath: path of the html file to parse\n",
    "    :param i: index which identifies the file\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "    title = get_title(soup)\n",
    "    intro = get_intro(soup)\n",
    "    plot = get_plot(soup)\n",
    "    infobox = get_infobox(soup)\n",
    "\n",
    "    out_path = tsv_path + '/info_' + i + '.tsv'\n",
    "    save_tsv(title, intro, plot, infobox, out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multithread process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the html parsing and tsv creation process faster, we decided to use multithread programming by creating the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multithread_create_tsv_files(movies_path, tsv_path, n_threads):\n",
    "    \"\"\"\n",
    "    This functions operates in multithread. It subdivides the list of files to parse into chucks and\n",
    "    let each thread call the function create_tsv_files over each chunk.\n",
    "    :param movies_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # list all the files to parse and divide the list into chunks\n",
    "    # depending on the number of threads to use.\n",
    "    files = os.listdir(movies_path)\n",
    "    l_chunks = N // n_threads\n",
    "    chunks = [files[i:i + l_chunks] for i in range(0, N, l_chunks)]\n",
    "\n",
    "    if len(chunks) > n_threads:\n",
    "        chunks[-2] += chunks[-1]\n",
    "        del chunks[-1]\n",
    "\n",
    "    threads = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    for i in range(n_threads):\n",
    "        # create n_threads threads, each one of each calls the create_tsv_files function\n",
    "        # on a different chunk of files.\n",
    "        threads.append(\n",
    "            Thread(name='Thread-' + str(i), target=create_tsv_files, args=[movies_path, tsv_path, chunks[i]]))\n",
    "\n",
    "    # start each thread\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "\n",
    "    # wait for each thread to finish its execution\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    minutes = (time.time() - start_time) / 60\n",
    "    print(\"---------------- TSV CREATED in %s minutes ------------------\" % minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the next line of code, the parsing process starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_path = data_path + r'\\movies'\n",
    "tsv_filepath = data_path + r'\\tsv_files'\n",
    "multithread_create_tsv_files(movies_path, tsv_filepath, n_threads=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to create two different Search Engines that, given as input a query, return the movies that match the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this moment, we narrow our interest on the *intro* and *plot* of each document. It means that the first Search Engine will evaluate queries with respect to the aforementioned information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the homework it has been requestd to create a **vocabulary** file that associates to each word a term_id. We created the functions to that, but at the end we decided to not use them and stick with the actual word to creat the index, since the difference in time was minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the functions to create the *vocabulary.json* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_into_vocabulary(text, vocab):\n",
    "    \"\"\"\n",
    "    Given a text and a vocab (dict), assigns to each word of the text and id.\n",
    "    :param text:\n",
    "    :param vocab:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    term_id = 0\n",
    "    if len(vocab) > 0:\n",
    "        keys = sorted(list(map(int, list(vocab.values()))), reverse=True)\n",
    "        term_id = keys[0] + 1\n",
    "\n",
    "    for word in text.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = term_id\n",
    "            term_id += 1\n",
    "\n",
    "\n",
    "def map_vocabulary(vocab_filepath, tsv_filepath):\n",
    "    \"\"\"\n",
    "    Performs the function map_into_vocabulary for each tsv file in tsv_filepath and then stores the vocab\n",
    "    into a json file.\n",
    "    :param vocab_filepath:\n",
    "    :param tsv_filepath:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    files = os.listdir(tsv_filepath)\n",
    "\n",
    "    try:\n",
    "        with open(vocab_filepath, 'r') as f:\n",
    "            vocab = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        vocab = {}\n",
    "\n",
    "    for filename in files:\n",
    "        print(filename)\n",
    "        text = get_text(tsv_filepath, filename)\n",
    "        map_into_vocabulary(text, vocab)\n",
    "\n",
    "    with open(vocab_filepath, 'w') as out:\n",
    "        json.dump(vocab, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions for the vocabulary creation have been grouped into the **index_utils.py** file.  \n",
    "By running the next lines of code you create the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_utils.map_vocabulary(vocab_filepath, tsv_filepath)\n",
    "print(\"---------------- Vocabulary Mapped --------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1) Create your index!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to define the function used to create the first *inverted_index*, which will associate to each word the list of tsv documents which contain it. Differently from the suggestion in the README we will not use the term_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need to define a function to clean the text after getting it from the tsv file.  \n",
    "To the text will be performed the following cleaning operations:  \n",
    "1) Removing stopwords  \n",
    "2) Removing punctuation  \n",
    "3) Stemming  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(s):\n",
    "    \"\"\"\n",
    "    Takes in input a string to be cleaned up.\n",
    "    Returns a new cleaned up string.\n",
    "    :param s\n",
    "    :return: cleaned up string\n",
    "    \"\"\"\n",
    "    import string\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import PorterStemmer\n",
    "\n",
    "    # Removing stopwords, punctuation and stemming\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "    punctuation.add('``')\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    s = s.replace('\\n', ' ')  # replacing new lines with a space\n",
    "    word_tokens = word_tokenize(s)\n",
    "    # Checking the each word is both not a stopword or punctuation and, after that,\n",
    "    # stemming and lowering it. It is important to lower each word because otherwise,\n",
    "    # in future steps, the search engine would consider for example \"Rome\" and \"rome\" two different words.\n",
    "    filtered_string = [ps.stem(w.lower()) for w in word_tokens if w not in stop_words and w not in punctuation]\n",
    "\n",
    "    return ' '.join(filtered_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *clean_up* function has been stored in the **utils.py** file since it will be called from external funcions too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to create the index, we need the text of each tsv file, defined in this case as intro + plot. Here is the function to get the text, stored into **index_utils.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(tsv_filepath, filename):\n",
    "    \"\"\"\n",
    "    Given in input the path to the folder containing the tsv files and the name of the file to parse.\n",
    "    Returns a string composed by intro + plot.\n",
    "    :param tsv_filepath:\n",
    "    :param filename:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    filepath = tsv_filepath + '/' + filename\n",
    "    tsv = pd.read_csv(filepath, delimiter='\\t')\n",
    "    intro = tsv.loc[0, 'intro']\n",
    "    plot = tsv.loc[0, 'plot']\n",
    "    text = utils.clean_up(str(intro) + \" \" + str(plot))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are able to create the first inverted index using the following function, stored into **index.py**. The index will be stored as *.json* file, so that we need to create it just one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(index_filepath, tsv_filepath):\n",
    "    \"\"\"\n",
    "    Takes in input the filepath of the inverted inverted index and the filepath of the folder containing\n",
    "    the tsv files.\n",
    "    Creates a file at the index_filepath which associates to each word found in any document, the list of\n",
    "    documents (tsv files) which contain it. Then stores the file on the drive.\n",
    "    :param index_filepath:\n",
    "    :param tsv_filepath:\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # open the index file if already exists or create and index dict otherwise\n",
    "    try:\n",
    "        with open(index_filepath, 'r') as f:\n",
    "            index = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        index = {}\n",
    "\n",
    "    # list all the tsv files\n",
    "    files = os.listdir(tsv_filepath)\n",
    "\n",
    "    for filename in files:\n",
    "        print(filename)\n",
    "        text = index_utils.get_text(tsv_filepath, filename)\n",
    "\n",
    "        for word in text.split():\n",
    "            index[word] = index.get(word, []) + [filename]\n",
    "\n",
    "    with open(index_filepath, 'w') as out:\n",
    "        json.dump(index, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the next lines of code you create the inverted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_filepath = data_path + r'\\index.json'\n",
    "index = index.create_inverted_index(index_filepath, tsv_filepath)\n",
    "print(\"---------------- Index Created --------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2) Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import os.path\n",
    "import re\n",
    "import time\n",
    "from threading import Thread\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a query, that you let the user enter, the Search Engine is supposed to return a list of documents. Since we are dealing with conjunctive queries (AND), each of the returned documents should contain all the words in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query_base(index, query, index_to_link_path, tsv_path):\n",
    "    # Creating a set of movies which contains all the words form the query\n",
    "    movies = set()\n",
    "    words_dict = dict()  # associates to each word the list of movies\n",
    "    words = query.split()\n",
    "    first_iter = True\n",
    "    for word in words:\n",
    "        if first_iter:\n",
    "            movies.update(index.get(word, []))\n",
    "            first_iter = False\n",
    "            continue\n",
    "        # with intersection_update we are able to maintain only the movies which contain all the word from the query\n",
    "        movies.intersection_update(index.get(word, []))\n",
    "\n",
    "    with open(index_to_link_path, 'r') as f:\n",
    "        index_to_link = json.load(f)\n",
    "\n",
    "    # create the final dict containing title, intro, url and similarity for each movie\n",
    "    results = dict()\n",
    "    for movie in movies:\n",
    "        filepath = tsv_path + '/' + str(movie)\n",
    "        tsv = pd.read_csv(filepath, delimiter='\\t')\n",
    "        # with open(filepath, 'r') as f:\n",
    "        #     tsv = csv.DictReader(f, dialect='excel-tab')\n",
    "        title = tsv.loc[0, 'title']\n",
    "        intro = tsv.loc[0, 'intro']\n",
    "        index = re.findall(r'\\d+', movie)[0]\n",
    "        url = index_to_link[str(index)]\n",
    "        results[index] = [title, intro, url]\n",
    "\n",
    "    # convert the dict to a pandas dataframe\n",
    "    table = pd.DataFrame(results, columns=['Title', 'Intro', 'Wikipedia Url'])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the next lines of code you obtain and execute the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = utils.clean_up(input(\"Please, write a query: \"))\n",
    "table = execute_query_base(index, query, index_to_link_path, tsv_filepath)\n",
    "print(table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section is going to be defined a new search engine that, given a query, returns the *top-k* documents related to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1) Inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need to create and *inverted_score_index*.  \n",
    "This index will associate to each word, a list tuple (document, tf, idf), with all the documents containing that word. We decided to store tf and idf separately instead of directly storing the tfidf since, when calculating the cosine similarity, we will need the idf for each word -> document association."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating the index we are going to parse the text (intro + plot) of each document and store all of them in a json file for easy and fast access in the future operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_docs_text(tsv_filepath, texts_filepath):\n",
    "    \"\"\"\n",
    "    Creates a json file which contains the association tsv_filename -> text (intro + plot).\n",
    "    Thanks to this file every time that a text is needed it is already stored into this json file.\n",
    "    :param tsv_filepath:\n",
    "    :param texts_filepath:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    files = os.listdir(tsv_filepath)\n",
    "    d = dict()\n",
    "\n",
    "    for f in files:\n",
    "        print(f)\n",
    "        d[f] = get_text(tsv_filepath, f)\n",
    "\n",
    "    with open(texts_filepath, 'w') as out:\n",
    "        json.dump(d, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_utils.save_all_docs_text(tsv_filepath, texts_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrive the file created we will use the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_filepath = data_path + r'\\texts.json'\n",
    "texts = get_texts_file(texts_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the function to calculate tf, idf and to create the scored index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(word, document):\n",
    "    \"\"\"\n",
    "    Computes the term frequency (tf) of word into document.\n",
    "    :param word: string\n",
    "    :param document: string\n",
    "    :return: tf\n",
    "    \"\"\"\n",
    "    occurrences = document.count(word)\n",
    "    return occurrences / len(document.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(N, df):\n",
    "    \"\"\"\n",
    "    Computes the inverse document frequency of a word.\n",
    "    :param N: total number of documents\n",
    "    :param df: number of documents which contain the word in question\n",
    "    :return: idf\n",
    "    \"\"\"\n",
    "    return math.log10(N / (df + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scored_index(index, N, texts, partial_indexes, i):\n",
    "    \"\"\"\n",
    "    Iterates over the index (portion of the whole inverted_index and, for each word of the index,\n",
    "    computes the tfidf of each document associated to the word.\n",
    "    :param index: inverted index in json format\n",
    "    :param N: total number of documents\n",
    "    :param texts: dict with associations doc_name -> text (intro + plot)\n",
    "    :param partial_indexes: list of partial indexes (used for the multithread operations)\n",
    "    :param i: index of the actual partial index\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    scored_index = {}\n",
    "    count = 1\n",
    "    for word, documents in index.items():\n",
    "        print(count)\n",
    "        scored_index[word] = []\n",
    "        df = len(documents)\n",
    "        for doc_name in documents:\n",
    "            text = texts[doc_name]\n",
    "            _tf = index_utils.tf(word, text)\n",
    "            _idf = index_utils.idf(N, df)\n",
    "            scored_index[word].append((doc_name, _tf, _idf))\n",
    "        count += 1\n",
    "\n",
    "    partial_indexes[i] = scored_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the functions stores the scored index just created as element of a list. This is beacause, as for the tsv file creation, this process gets handled in multithread. This allows the index creation process to be much faster. Here is how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multithread_create_scored_index(_index, scored_index_filepath, N, texts, n_threads):\n",
    "    \"\"\"\n",
    "    This functions operates in multithread. It subdivides the items of the initial inverted index\n",
    "    into chucks depending on the number of threads to use and let each thread call the function\n",
    "    create_scored_index over each chunk.\n",
    "    :param _index:\n",
    "    :param scored_index_filepath:\n",
    "    :param N:\n",
    "    :param texts:\n",
    "    :param n_threads:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # calculate the number of chunks and subdivide the initial index\n",
    "    l_chunks = len(_index) // n_threads\n",
    "    items = list(_index.items())\n",
    "    chunks = [dict(items[i:i + l_chunks]) for i in range(0, len(_index), l_chunks)]\n",
    "\n",
    "    if len(chunks) > n_threads:\n",
    "        chunks[-2].update(chunks[-1])\n",
    "        del chunks[-1]\n",
    "\n",
    "    threads = []\n",
    "    partial_indexes = [{} for i in range(n_threads)]  # each partial scored_index will be saved in this list\n",
    "\n",
    "    start_time = time.time()\n",
    "    for i in range(n_threads):\n",
    "        # create the threads with the target function create_scored_index and the parameters.\n",
    "        threads.append(Thread(name='Thread-' + str(i),\n",
    "                              target=index.create_scored_index,\n",
    "                              args=[chunks[i], N, texts, partial_indexes, i]))\n",
    "\n",
    "    # start each thread\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "\n",
    "    # wait for each thread to finish its execution\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    # join all the partial scored indexes into a unique index\n",
    "    scored_index = dict()\n",
    "    for d in partial_indexes:\n",
    "        scored_index.update(d)\n",
    "\n",
    "    # remove eventual duplicated elements\n",
    "    for word, lst in scored_index.items():\n",
    "        lst = list(set(lst))\n",
    "        scored_index[word] = lst\n",
    "\n",
    "    with open(scored_index_filepath, 'w') as out:\n",
    "        json.dump(scored_index, out)\n",
    "\n",
    "    minutes = (time.time() - start_time) / 60\n",
    "    print(\"---------------- SCORED INDEX created in %s minutes ------------------\" % minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the next section the scored inverted index gets created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_index_filepath = data_path + r'\\scored_index.json'\n",
    "N = len(os.listdir(movies_path)) # number of total documents\n",
    "_index = index_utils.get_index(index_filepath)\n",
    "texts = get_texts_file(texts_filepath)\n",
    "multithread_create_scored_index(_index, scored_index_filepath, N, texts, n_threads=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2) Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the scored index we can define the functions for the execution of the query.  \n",
    "In this section, Cosine Similarity is used to see how much the query is equal to the document found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the Cosine Similarity between each document and the query we need to be able to compute the **dot product** between 2 list of floats and the **norm** of a list of floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(a1, a2):\n",
    "    \"\"\"\n",
    "    Given 2 lists of integers a1 and a2, performs the dot product between the 2.\n",
    "    :param a1:\n",
    "    :param a2:\n",
    "    :return: dot product\n",
    "    \"\"\"\n",
    "    s = 0\n",
    "    if len(a1) != len(a2):\n",
    "        raise Exception('Arrays have not the same length!')\n",
    "    for i in range(len(a1)):\n",
    "        s += a1[i] * a2[i]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(a):\n",
    "    \"\"\"\n",
    "    Given the list of integers a, performs the norm of the list.\n",
    "    :param a:\n",
    "    :return: norm of a\n",
    "    \"\"\"\n",
    "    s = 0\n",
    "    for num in a:\n",
    "        s += (float(num) ** 2)\n",
    "    return np.sqrt(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are able to compute the cosine similarity between the query and each documents containing all the words in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(query, words_dict):\n",
    "    \"\"\"\n",
    "    Given a query and a dict words_dict, which associates to each word a list of (doc, tf, idf) for that word,\n",
    "    returns a dict doc_cosine which associates to each documents the cosine similarity to the query.\n",
    "    :param query: string\n",
    "    :param words_dict: dict\n",
    "    :return: doc_cosine dict\n",
    "    \"\"\"\n",
    "    words = query.split()\n",
    "\n",
    "    # for each word in the query, calculate its term frequency and put the association in a dict.\n",
    "    # then for each document containing the word, get the tf and idf and put it in new lists. In the end\n",
    "    # associate the lists to the doc in new dicts doc_to_word.\n",
    "    word_score = {}\n",
    "    doc_to_word_tf = {}\n",
    "    doc_to_word_idf = {}\n",
    "    for word in words:\n",
    "        word_score[word] = index_utils.tf(word, query)\n",
    "        for (doc, _tf, _idf) in words_dict[word]:\n",
    "            l = doc_to_word_tf.get(doc, [])\n",
    "            l.append(_tf*_idf)\n",
    "            doc_to_word_tf[doc] = l\n",
    "            l1 = doc_to_word_idf.get(doc, [])\n",
    "            l1.append(_idf)\n",
    "            doc_to_word_idf[doc] = l1\n",
    "\n",
    "    # for each association (doc, tfidf_vec) in the doc_to_word dict, compute the cosine similarity\n",
    "    # between the doc and the query, given the vector containing the tf's of each word in the query.\n",
    "    doc_cosine = {}\n",
    "    query_tf_vec = list(word_score.values())\n",
    "    for (doc, tfidf_vec) in doc_to_word_tf.items():\n",
    "        query_tfidf_vec = [query_tf_vec[i]*doc_to_word_idf[doc][i] for i in range(len(query_tf_vec))]\n",
    "        doc_cosine[doc] = dot(tfidf_vec, query_tfidf_vec) / (norm(query_tfidf_vec) * norm(tfidf_vec))\n",
    "\n",
    "    return doc_cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define some functions the we will need for the execution of the query search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_elements(lst, k):\n",
    "    \"\"\"\n",
    "    Given in input the list lst, removes the elements that are repeated less than k times.\n",
    "    :param lst:\n",
    "    :param k:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    counted = Counter(x[0] for x in lst)\n",
    "\n",
    "    temp_lst = []\n",
    "    for el in counted:\n",
    "        if counted[el] < k:\n",
    "            temp_lst.append(el)\n",
    "\n",
    "    res_lst = []\n",
    "    for (el, tf, idf) in lst:\n",
    "        if el not in temp_lst and el not in res_lst:\n",
    "            res_lst.append(el)\n",
    "\n",
    "    return res_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_items(results, k):\n",
    "    \"\"\"\n",
    "    Takes in input a dictionary of results (index: list of info) and an integer k > 0.\n",
    "    Orders the items of the dict using a heap structure and returns a list with the first k movies.\n",
    "    :param results: dict\n",
    "    :param k: int\n",
    "    :return: dict\n",
    "    \"\"\"\n",
    "    from heapq import _heapify_max\n",
    "\n",
    "    # put every items in a tuple (similarity, index) and append to a new list\n",
    "    lst = []\n",
    "    for (index, info) in results.items():\n",
    "        lst.append((info[-1], index))\n",
    "\n",
    "    # order the list using an heap data structure and take the first k items\n",
    "    ordered_list = []\n",
    "    for i in range(k):\n",
    "        try:\n",
    "            _heapify_max(lst)\n",
    "            ordered_list.append(lst[0])\n",
    "            lst.pop(0)\n",
    "        except: # if there are less than k items, just keep the ones found\n",
    "            break\n",
    "\n",
    "    # put the items in a new list and return it\n",
    "    best_items = []\n",
    "    for sim, index in ordered_list:\n",
    "        best_items.append(results[index])\n",
    "\n",
    "    return best_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the function for the query execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query_cosine(index, query, index_to_link_path, tsv_path, k):\n",
    "    \"\"\"\n",
    "    This function takes in input a scored_index and a query and prints the 5 movies\n",
    "    with most similarity to the query.\n",
    "    :param index:\n",
    "    :param query:\n",
    "    :return: dataframe with results\n",
    "    \"\"\"\n",
    "    # Creating a set of movies which contains all the words form the query\n",
    "    movies = []\n",
    "    words_dict = dict()  # associates to each word the list of (doc, score)\n",
    "    words = query.split()\n",
    "    n_words = len(words)\n",
    "    for word in words:\n",
    "        movies.extend(index.get(word, []))\n",
    "        words_dict[word] = index.get(word, [])\n",
    "\n",
    "    # keep only the movies that contain all the words from the query\n",
    "    relevant_movies = utils.remove_elements(movies, n_words)\n",
    "    for (word, lst) in words_dict.items():\n",
    "        temp = []\n",
    "        for (doc, tf, idf) in lst:\n",
    "            if doc in relevant_movies:\n",
    "                temp.append((doc, tf, idf))\n",
    "        words_dict[word] = temp\n",
    "\n",
    "    # calculate the cosine similarity between each movie and the query\n",
    "    # and store it in a dict\n",
    "    cosine_similarities = similarities_utils.cosine_similarity(query, words_dict)\n",
    "\n",
    "    with open(index_to_link_path, 'r') as f:\n",
    "        index_to_link = json.load(f)\n",
    "\n",
    "    # create the final dict containing title, intro, url and similarity for each movie\n",
    "    results = dict()\n",
    "    for movie in relevant_movies:\n",
    "        filepath = tsv_path + '/' + str(movie)\n",
    "        tsv = pd.read_csv(filepath, delimiter='\\t')\n",
    "        # with open(filepath, 'r') as f:\n",
    "        #     tsv = csv.DictReader(f, dialect='excel-tab')\n",
    "        title = tsv.loc[0, 'title']\n",
    "        intro = tsv.loc[0, 'intro']\n",
    "        index = re.findall(r'\\d+', movie)[0]\n",
    "        url = index_to_link[str(index)]\n",
    "        similarity = round(cosine_similarities[movie], 2)\n",
    "        results[index] = [title, intro, url, similarity]\n",
    "\n",
    "    # get the first k items with most similarity to the query\n",
    "    first_items = utils.get_top_items(results, k)\n",
    "\n",
    "    # convert the dict to a pandas dataframe\n",
    "    table = pd.DataFrame(first_items, columns=['Title', 'Intro', 'Wikipedia Url', 'Similarity'])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next section to input and execute a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = utils.clean_up(input(\"Please, write a query: \"))\n",
    "k = -1\n",
    "while k < 0:\n",
    "    k = int(input(\"Choose the maximum number of results to show: \"))\n",
    "index = index_utils.get_index(scored_index_filepath)\n",
    "print(\"Cosine Similarity Results\")\n",
    "table = execute_query_cosine(index, query, index_to_link_path, tsv_filepath, k)\n",
    "print(table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a new score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to sort them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the relevants movies, to define a new score we work in terms of sets. First of all create a dataframe containing all the variables you need (title, intro, plot, language, producer, etc..). \n",
    "\n",
    "Then, given a query, see if in each variable (for each film) there is the required word in this way:\n",
    "\\begin{equation*}\n",
    "\\frac {A \\cap B}{A \\cup B}\n",
    "\\end{equation*}\n",
    "Where A is a set composed of the words of the query, while B is the set of words in each cell of the dataframe.\n",
    "\n",
    "This is called Jaccard index and is between 0 and 1.\n",
    "\n",
    "Now, for each cell of the dataframe there is a Jaccard index  which gives us a 'frequency' of the single words of the query for each cell of the dataframe. Then do a sum of each row (that contains all the jaccard indeces for a film) to see a new index between 0 and the numbers of variables. If this new score is highest then the film is the most relevant.\n",
    "\n",
    "The last step is to normalize the array containing the new score between 0 and 1. In this way the closest film to 1 will be the most similar to the query among the relevant films, the one closest to 0 will be the least similar among the films relevant (does not therefore mean that it is not relevant).\n",
    "\n",
    "This algorithm works because if in the query there is a word of the title, automatically that film will be among the first on the list because the Jaccard index will be high.\n",
    "Similarly, if there is a date it will be particularly important because the 'date' cell contains few words.\n",
    "So if it finds a query word in cells that contain many words it tends to take it very little into consideration, as opposed to cells with few words that (rightly) have more importance for a search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe with only the movies that contain all the words from the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(relevant_movies, tsv_filepath, index_to_link_filepath):\n",
    "    cols = ['title', 'intro', 'plot', 'director', 'producer', 'writer', 'starring', 'music', 'release date', 'runtime',\n",
    "            'country', 'language', 'budget', 'url']\n",
    "    final_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    with open(index_to_link_filepath, 'r') as f:\n",
    "        index_to_link = json.load(f)\n",
    "\n",
    "    for movie in relevant_movies:\n",
    "        temp_df = pd.read_csv(tsv_filepath + '/' + movie, delimiter='\\t')\n",
    "        index = re.findall(r'\\d+', movie)[0]\n",
    "        url = index_to_link[str(index)]\n",
    "        temp_df['url'] = [url]\n",
    "        final_df = pd.concat([final_df, temp_df], sort=True)\n",
    "\n",
    "    return final_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate an array containing the sum of the Jaccard indeces for each relevant film."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard similarity give us an index between 0 and 1\n",
    "def jaccard_similarity(_query, _text):\n",
    "    _query = set(_query.split())\n",
    "    _text = set(_text.split())\n",
    "    return len(_query.intersection(_text)) / len(_query.union(_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then normalize it between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(values, actual_bounds, desired_bounds):\n",
    "    return [desired_bounds[0] + (x - actual_bounds[0]) * (desired_bounds[1] - desired_bounds[0]) / (actual_bounds[1] - actual_bounds[0]) for x in values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is defined the function to execute the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query_jaccard(index, query, index_to_link_path, tsv_filepath, k):\n",
    "    # Creating a set of movies which contains all the words form the query\n",
    "    movies = []\n",
    "    words_dict = dict()  # associates to each word the list of (doc, score)\n",
    "    words = query.split()\n",
    "    n_words = len(words)\n",
    "    for word in words:\n",
    "        movies.extend(index.get(word, []))\n",
    "        words_dict[word] = index.get(word, [])\n",
    "\n",
    "    # keep only the movies that contain all the words from the query\n",
    "    relevant_movies = utils.remove_elements(movies, n_words)\n",
    "\n",
    "    dataframe = similarities_utils.get_dataframe(relevant_movies, tsv_filepath, index_to_link_path)\n",
    "\n",
    "    jac_score_final = []\n",
    "    # Jaccard index for each position\n",
    "    cols = dataframe.columns\n",
    "    for index, row in dataframe.iterrows():\n",
    "        jac_score = []\n",
    "        for col in cols:\n",
    "            text = str(row[col])\n",
    "            jac_score.append(similarities_utils.jaccard_similarity(query, utils.clean_up(text)))\n",
    "        jac_score_final.append(sum(jac_score))\n",
    "\n",
    "    if len(jac_score_final) > 1:\n",
    "        _jac = similarities_utils.normalize(jac_score_final,\n",
    "                                            (min(jac_score_final), max(jac_score_final)),\n",
    "                                            (0, 1))\n",
    "    else:\n",
    "        _jac = jac_score_final\n",
    "\n",
    "    dataframe['similarity'] = list(map(lambda x: round(x, 2), _jac))\n",
    "\n",
    "    results = dict()\n",
    "    for index, row in dataframe.iterrows():\n",
    "        title = row['title']\n",
    "        intro = row['intro']\n",
    "        url = row['url']\n",
    "        sim = row['similarity']\n",
    "        results[index] = [title, intro, url, sim]\n",
    "\n",
    "    # get the first k items with most similarity to the query\n",
    "    first_items = utils.get_top_items(results, k)\n",
    "\n",
    "    # convert the dict to a pandas dataframe\n",
    "    table = pd.DataFrame(first_items, columns=['Title', 'Intro', 'Wikipedia Url', 'Similarity'])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next section to input and execute a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = utils.clean_up(input(\"Please, write a query: \"))\n",
    "k = -1\n",
    "while k < 0:\n",
    "    k = int(input(\"Choose the maximum number of results to show: \"))\n",
    "index = index_utils.get_index(scored_index_filepath)\n",
    "print(\"Jaccard Similarity Results\")\n",
    "table = execute_query_jaccard(index, query, index_to_link_path, tsv_filepath, k)\n",
    "print(table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Professionality is the key!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be a little bit more professional, here is a section of code that permits the user to choose the desidered type of search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"\\nChoose between the following search engines:\")\n",
    "    print(\"1 - Base Search Engine\")\n",
    "    print(\"2 - Search Engine based on Cosine Similarity\")\n",
    "    print(\"3 - Search Engine based on Jaccard Similarity\")\n",
    "    choice = int(input(\"Enter your choice: \"))\n",
    "    while choice not in [1, 2, 3]:\n",
    "        print(\"Please, insert a valid choice!\")\n",
    "        choice = int(input(\"Enter your choice: \"))\n",
    "\n",
    "    query = utils.clean_up(input(\"Please, write a query: \"))\n",
    "\n",
    "    if choice == 1:\n",
    "        index = index_utils.get_index(index_filepath)\n",
    "        table = execute_query_base(index, query, index_to_link_path, tsv_filepath)\n",
    "    else:\n",
    "        k = -1\n",
    "        while k < 0:\n",
    "            k = int(input(\"Choose the maximum number of results to show: \"))\n",
    "        index = index_utils.get_index(scored_index_filepath)\n",
    "        if choice == 2:\n",
    "            print(\"Cosine Similarity Results\")\n",
    "            table = execute_query_cosine(index, query, index_to_link_path, tsv_filepath, k)\n",
    "        elif choice == 3:\n",
    "            print(\"Jaccard Similarity Results\")\n",
    "            table = execute_query_jaccard(index, query, index_to_link_path, tsv_filepath, k)\n",
    "    print(table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Algorithmic question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function take in input three values: string, first value and last value. If the string is composed just by one value then the longest substring palindrome will be 1. If the string is composed by two values and these values are equal, then the longest substring palindrome will be 2.\n",
    "\n",
    "For strings with a length bigger than 2 is used a recursive function. If first and last value are equal then recall the function and move the first value to the second and the last value to the previous one. Also add 2 to the final result as the values are the same\n",
    "\n",
    "If those values aren't equal, the same function is recalled, first moving the last value from the last current to the previous one, then trying moving the first value from the first current to the next. So take the biggest result from those recursive function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_palindrome(word, i, k):  # i=word[0]  k=word[-1]\n",
    "\n",
    "    if (i == k):  # len(word)==1\n",
    "        return 1\n",
    "\n",
    "    if (word[i] == word[k] and i + 1 == k):  # len(word)==2 and word[0]==word[1]\n",
    "        return 2\n",
    "\n",
    "    if (word[i] == word[k]):                        # word[0]==word[-1]\n",
    "        return max_palindrome(word, i + 1, k - 1) + 2  # recall the function and\n",
    "                                                       # add 2 for count the previous letters that are equal\n",
    "\n",
    "    res1 = max_palindrome(word, i, k - 1)\n",
    "    res2 = max_palindrome(word, i + 1, k)\n",
    "\n",
    "    if res1 > res2:\n",
    "        return res1\n",
    "    else:\n",
    "        return res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert a sequence of characters: \n",
      "itopinonavevanonipoti\n",
      "Max palindrome subsequence has length:  21\n"
     ]
    }
   ],
   "source": [
    "print('Insert a sequence of characters: ')\n",
    "word = input().strip()\n",
    "n = len(word)\n",
    "print(\"Max palindrome subsequence has length: \",\n",
    "      max_palindrome(word, 0, n - 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}